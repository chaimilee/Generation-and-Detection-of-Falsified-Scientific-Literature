{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNBqhwVdkHNs",
        "outputId": "35abe759-951c-4a48-d0c2-7bb816a7143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grover'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Total 104 (delta 0), reused 0 (delta 0), pack-reused 104\u001b[K\n",
            "Receiving objects: 100% (104/104), 672.23 KiB | 2.74 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-28-15b8905d83af>\", line 2, in <module>\n",
            "    get_ipython().magic('cd /content/grover')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-84>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/magics/osm.py\", line 288, in cd\n",
            "    oldcwd = py3compat.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "!cd /content && rm -rf grover && git clone https://github.com/rowanz/grover.git\n",
        "%cd /content/grover\n",
        "!pip install -r requirements-gpu.txt\n",
        "!python download_model.py large"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from textwrap import wrap"
      ],
      "metadata": {
        "id": "fTc6VT6KkSvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install tensorboard==2.8.0\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "metadata": {
        "id": "EXoYhR4skT23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tika"
      ],
      "metadata": {
        "id": "wwyMWLdCkbjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e712a5-222d-448d-efb9-3256141f1b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=d45e1dedf8ff905cbefd2e9a189fa8de601e8785256c0a9bed76aa5efbc5547e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tika funtions for text extraction\n",
        "from tika import parser as p\n",
        "import pandas as pd\n",
        "import requests\n",
        "    \n",
        "#function pdf -> text extraction\n",
        "def get_data_from_web(url):\n",
        "    response = requests.get(url)\n",
        "    results = p.from_buffer(response.content)\n",
        "    return results\n",
        "\n",
        "def get_data_from_given_path(file_path):\n",
        "    results = p.from_file(file_path)\n",
        "    return results\n",
        "def extract_text(file_path):\n",
        "    final_text = []\n",
        "    #extract text from any website pdf\n",
        "    #pdf_url = \"https://www.bl.uk/learning/resources/pdf/makeanimpact/sw-transcripts.pdf\"\n",
        "    #results = get_data_from_web(pdf_url)\n",
        "    #print(\"\\nFile Content: \\n{}\".format(results[\"content\"].strip()))\n",
        "    pdf_file_path = file_path\n",
        "    results = get_data_from_given_path(pdf_file_path)\n",
        "    text = results[\"content\"].strip()\n",
        "    #final_text = [string for string in text if string != \"\"]\n",
        "    final_text = results['metadata']\n",
        "    return text\n",
        "    #print(results[\"content\"].strip())"
      ],
      "metadata": {
        "id": "rAOldMYZkcei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Final_bik_dataset.tsv', sep='\\t',encoding = \"ISO-8859-1\", nrows=214)"
      ],
      "metadata": {
        "id": "FjZMZvkDkg1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert authors to a list of strings\n",
        "def author_list(authors):\n",
        "  x=authors.split(\", \")\n",
        "  return list(x)"
      ],
      "metadata": {
        "id": "tMkX6L2lkn1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authors=df['Authors'].tolist()\n",
        "new_authors=[]\n",
        "\n",
        "\n",
        "for element in authors:\n",
        "  new_element=author_list(element)\n",
        "  new_authors.append(new_element)\n",
        "\n",
        "#print(new_authors)\n",
        "\n",
        "df['new_authors']=new_authors"
      ],
      "metadata": {
        "id": "25iDluJlkqPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert date into grover format\n",
        "year=df['Year'].tolist()\n",
        "month=df['Month'].tolist()\n",
        "def month_conversion(month):\n",
        "  try:\n",
        "    new_month=month.split(\"-\")\n",
        "    new_month_num=new_month[0]\n",
        "    new_month_num=int(new_month_num)\n",
        "    new_month='0'+str(new_month_num)\n",
        "  except:\n",
        "    new_month='06'\n",
        "  return new_month\n",
        "\n",
        "new_month_list=[]\n",
        "for index in range(len(month)):\n",
        "  new_month_list.append(month_conversion(month[index]))"
      ],
      "metadata": {
        "id": "gBRJpee6k0MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates_list=[]\n",
        "for index in range(len(year)):\n",
        "  date=str(year[index])+'-'+str(new_month_list[index])+'-01'\n",
        "  dates_list.append(date)"
      ],
      "metadata": {
        "id": "HD3RxL_-lAzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get downloaded pdfs from Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Tplt7ffUoTo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f48ff75-3fb3-4078-fc9e-a87211c9eee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "8AiqNSZqvmw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_by_dash(name):\n",
        "  new_name=name.split(\".\")\n",
        "  return new_name[0]"
      ],
      "metadata": {
        "id": "ay0iyvUbxD8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remane all files in pdfs folder\n",
        "folder=\"/content/drive/MyDrive/DSCI 550/pdfs\"\n",
        "\n",
        "for count, filename in enumerate(os.listdir(folder)):\n",
        "    dst = separate_by_dash(filename)+\".pdf\"\n",
        "    src =f\"{folder}/{filename}\"  # foldername/filename, if .py file is outside folder\n",
        "    dst =f\"{folder}/{dst}\"\n",
        "\n",
        "    os.rename(src, dst)"
      ],
      "metadata": {
        "id": "LcsId8-2vnI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a list of input jsons to feed into grover\n",
        "titles=df['Title'].tolist()\n",
        "\n",
        "input_json_list=[]\n",
        "\n",
        "for index in range(len(titles)):\n",
        "  publish_date=dates_list[index]\n",
        "  pdf_index=index+2\n",
        "  new_pdf_name=str(pdf_index)\n",
        "  #print(new_pdf_name)\n",
        "  try: #iff paper exists and is the name of the doi\n",
        "    input_json={}\n",
        "    input_json['title']=titles[index]\n",
        "    input_json['domain']='https://libraries.usc.edu/'\n",
        "    input_json['text']=extract_text('/content/drive/MyDrive/DSCI 550/pdfs/'+new_pdf_name+'pdf.pdf') #deffinitely use try/except here\n",
        "    input_json['summary']=''\n",
        "    input_json['authors']=new_authors[index]\n",
        "    publish_date=dates_list[index]\n",
        "    grover_publish_date = datetime.strptime(publish_date, \"%Y-%m-%d\").strftime(\"%m-%d-%Y\")\n",
        "    input_json['publish_date']=grover_publish_date\n",
        "\n",
        "    input_json_list.append(input_json)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Xso0lSyZlBcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a list of fake texts\n",
        "fake_text_final=[]"
      ],
      "metadata": {
        "id": "e_rS-qEg3x4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through list of jsons to generate fake texts\n",
        "fake_texts=[]\n",
        "WRAP_EVERY_N_CHARACTERS = 100\n",
        "for element in input_json_list:\n",
        "  print(1)\n",
        "  input_json=element\n",
        "  \n",
        "  #grover\n",
        "  with open('./input.json', 'w') as f:\n",
        "    f.write(json.dumps(input_json))\n",
        "\n",
        "  !PYTHONPATH=$(pwd) python sample/contextual_generate.py -model_config_fn lm/configs/large.json \\\n",
        "    -model_ckpt models/large/model.ckpt \\\n",
        "    -metadata_fn input.json \\\n",
        "    -out_fn output.json\n",
        "  \n",
        "  with open('./output.json') as f:\n",
        "    file_contents = f.read()\n",
        "    data = json.loads(file_contents)\n",
        "\n",
        "  generated_article = data['gens_article'][0]\n",
        "  wrapped_article = '\\n'.join(wrap(generated_article, WRAP_EVERY_N_CHARACTERS))\n",
        "  #print(wrapped_article)\n",
        "  fake_texts.append(wrapped_article)\n"
      ],
      "metadata": {
        "id": "frIChG4flW4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#put 121 fake texts into a dataframe and convert into txt file (note to do this 5 times to get 500 texts)\n",
        "df_text=pd.DataFrame()\n",
        "\n",
        "df_text['texts']=fake_texts"
      ],
      "metadata": {
        "id": "8hYfow2n2wxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.to_csv('/content/pandas_t.txt', sep='\\t', mode='a')"
      ],
      "metadata": {
        "id": "BTOfDA3Q4YPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now compile 500 fake texts into one txt file\n",
        "df1=pd.read_csv('/content/pandas_t.txt',sep='\\t',nrows=214)\n",
        "df2=pd.read_csv('/content/pandas_t2.txt',sep='\\t',nrows=214)\n",
        "df3=pd.read_csv('/content/pandas_t3.txt',sep='\\t',nrows=214)\n",
        "df4=pd.read_csv('/content/pandas_t4.txt',sep='\\t',nrows=214)\n",
        "df5=pd.read_csv('/content/pandas_t5.txt',sep='\\t',nrows=214)"
      ],
      "metadata": {
        "id": "Uwq58Beb2w6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first=df1['texts'].tolist()\n",
        "second=df2['texts2'].tolist()\n",
        "third=df3['texts3'].tolist()\n",
        "fourth=df4['texts4'].tolist()\n",
        "fifth=df5['texts5'].tolist()\n",
        "fifth=fifth[:16]"
      ],
      "metadata": {
        "id": "401knVbJ2xCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first.extend(second)\n",
        "first.extend(third)\n",
        "first.extend(fourth)\n",
        "first.extend(fifth)"
      ],
      "metadata": {
        "id": "ONZqYIhm2xLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_texts=pd.DataFrame()\n",
        "\n",
        "df_texts['texts']=first"
      ],
      "metadata": {
        "id": "f8SI48u32xTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_texts.to_csv('/content/500_texts.txt', sep='\\t', mode='a')"
      ],
      "metadata": {
        "id": "EUxncjnu2xaS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c88c4c19-1274-4b7a-efd8-056bcd93f986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-74b821ab498f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/500_texts.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_texts' is not defined"
          ]
        }
      ]
    }
  ]
}